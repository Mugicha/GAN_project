{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=100, output_dim=1024))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(128*7*7))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Reshape((7, 7, 128), input_shape=(128*7*7,)))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Conv2D(64, (5, 5), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Conv2D(1, (5, 5), padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (5, 5), padding='same', input_shape=(28, 28, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, (5, 5)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    print(\"model.names: \" + model.layers[3].name)\n",
    "    [weights, bias] = model.layers[3].get_weights()\n",
    "    print(\"weights.shape: \" + str(weights.shape))\n",
    "    print(\"bias.shape: \" + str(bias.shape))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_containing_discriminator(g, d):\n",
    "    model = Sequential()\n",
    "    model.add(g)\n",
    "    d.trainable = False\n",
    "    model.add(d)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[1:3]\n",
    "    image = np.zeros((height*shape[0], width*shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = img[:, :, 0]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(BATCH_SIZE):\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "    X_train = X_train[:, :, :, None]\n",
    "    X_test = X_test[:, :, :, None]\n",
    "    # X_train = X_train.reshape((X_train.shape, 1) + X_train.shape[1:])\n",
    "    d = discriminator_model()\n",
    "    g = generator_model()\n",
    "    d_on_g = generator_containing_discriminator(g, d)\n",
    "    # d_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "    # g_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "    d_optim = Adam(lr=0.0005)\n",
    "    g_optim = Adam(lr=0.0005)\n",
    "    # g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    g.compile(loss='binary_crossentropy', optimizer=\"Adam\")\n",
    "    d_on_g.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "    d.trainable = True\n",
    "    d.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "    for epoch in range(1):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n",
    "        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
    "            noise = np.random.uniform(-1, 1, size=(BATCH_SIZE, 100))\n",
    "            image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "            generated_images = g.predict(noise, verbose=0)\n",
    "            # Save img.\n",
    "            \"\"\"\n",
    "            if index % 500 == 0:\n",
    "                image = combine_images(generated_images)\n",
    "                image = image*127.5+127.5\n",
    "                Image.fromarray(image.astype(np.uint8)).save(\n",
    "                    os.path.join('image', str(epoch)+\"_\"+str(index)+\".png\"))\n",
    "            \"\"\"\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            y = [1] * BATCH_SIZE + [0] * BATCH_SIZE\n",
    "            d_loss = d.train_on_batch(X, y)\n",
    "            print(\"batch %d d_loss : %f\" % (index, d_loss))\n",
    "            noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "            d.trainable = False\n",
    "            g_loss = d_on_g.train_on_batch(noise, [1] * BATCH_SIZE)\n",
    "            d.trainable = True\n",
    "            print(\"batch %d g_loss : %f\" % (index, g_loss))\n",
    "            if index % 10 == 9:\n",
    "                g.save_weights('generator', True)\n",
    "                d.save_weights('discriminator', True)\n",
    "    # Get feature map and output.\n",
    "    x = X_train[0]\n",
    "    return x, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(BATCH_SIZE, nice=False):\n",
    "    g = generator_model()\n",
    "    g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    g.load_weights('generator')\n",
    "    if nice:\n",
    "        d = discriminator_model()\n",
    "        d.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "        d.load_weights('discriminator')\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE*20, 100))\n",
    "        generated_images = g.predict(noise, verbose=1)\n",
    "        d_pret = d.predict(generated_images, verbose=1)\n",
    "        index = np.arange(0, BATCH_SIZE*20)\n",
    "        index.resize((BATCH_SIZE*20, 1))\n",
    "        pre_with_index = list(np.append(d_pret, index, axis=1))\n",
    "        pre_with_index.sort(key=lambda x: x[0], reverse=True)\n",
    "        nice_images = np.zeros((BATCH_SIZE,) + generated_images.shape[1:3], dtype=np.float32)\n",
    "        nice_images = nice_images[:, :, :, None]\n",
    "        for i in range(BATCH_SIZE):\n",
    "            idx = int(pre_with_index[i][1])\n",
    "            nice_images[i, :, :, 0] = generated_images[idx, :, :, 0]\n",
    "        image = combine_images(nice_images)\n",
    "    else:\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "        generated_images = g.predict(noise, verbose=1)\n",
    "        image = combine_images(generated_images)\n",
    "    image = image*127.5+127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\n",
    "        \"generated_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--mode\", type=str)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--nice\", dest=\"nice\", action=\"store_true\")\n",
    "    parser.set_defaults(nice=False)\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    args = get_args()\n",
    "    if args.mode == \"train\":\n",
    "        train(BATCH_SIZE=args.batch_size)\n",
    "    elif args.mode == \"generate\":\n",
    "        generate(BATCH_SIZE=args.batch_size, nice=args.nice)\n",
    "    \"\"\"\n",
    "    x, d = train(BATCH_SIZE=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(d.layers[3].get_weights()[0]))  # weights\n",
    "print(len(d.layers[3].get_weights()[1]))  # bias of each filter\n",
    "weights = np.array(d.layers[3].get_weights()[0]) # shape: (5, 5, 64, 128) = (kernel_h, kernel_w, input data num, num of filter)\n",
    "print(weights[:,:,0,:].shape)\n",
    "\n",
    "feature_imgs = []\n",
    "for f in np.split(weights[:,:,63,:], 128, axis=2):\n",
    "    f = image.array_to_img(f)  # 特徴マップを画像化する。\n",
    "    f = np.array(f)  # PIL オブジェクトを numpy 配列にする。\n",
    "    feature_imgs.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = 2\n",
    "rows = int(len(feature_imgs) / cols)\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(12, 100)) # cols * 2\n",
    "\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "        i = r * cols + c\n",
    "        f_axis = axes[r, c]\n",
    "        f_axis.imshow(feature_imgs[i], cmap='gray')\n",
    "        f_axis.set_title('Filter ' + str(r*2 + c))\n",
    "        f_axis.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
